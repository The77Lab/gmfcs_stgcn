{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "# Get the path of the parent directory\n",
    "parent_dir = os.path.dirname(script_dir)\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "import numpy as np\n",
    "\n",
    "from models.heads import Classifier\n",
    "from models.stgcn import STGCN\n",
    "from utils.data_processing import Preprocess_Module\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "\n",
    "transform = Preprocess_Module(data_augmentation=False)\n",
    "\n",
    "from network import STGCN_Classifier_Metric\n",
    "\n",
    "\n",
    "sample_folder = \"sample_folder here\"\n",
    "\n",
    "backbone_cfg = {\n",
    "    'type': 'STGCN',\n",
    "    'gcn_adaptive': 'init',\n",
    "    'gcn_with_res': True,\n",
    "    'tcn_type': 'mstcn',\n",
    "    'num_stages': 10,\n",
    "    'inflate_stages': [5, 8],\n",
    "    'down_stages': [5, 8],\n",
    "    'graph_cfg': {\n",
    "        'layout': 'coco',\n",
    "        'mode': 'spatial'\n",
    "    },\n",
    "    'pretrained': None\n",
    "}\n",
    "\n",
    "model = STGCN_Classifier_Metric(backbone=backbone_cfg, num_classes=4)\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "def load_sample(video, clip):\n",
    "    data_file_path = os.path.join(sample_folder, str(video), str(video) + '_' + str(clip) + '.npy')\n",
    "    data = np.load(data_file_path)\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['img_shape'] = (320, 480)\n",
    "    tmp_dict['label'] = -1\n",
    "    tmp_dict['start_index'] = 0\n",
    "    tmp_dict['modality'] = 'Pose'\n",
    "    tmp_dict['total_frames'] = 124\n",
    "    data = np.where(data == 0, 1e-4, data)\n",
    "    data[np.isnan(data)] = 1e-4\n",
    "    tmp_dict['keypoint'] = data[np.newaxis, :, :, :2]\n",
    "    tmp_dict['keypoint'] = np.tile(tmp_dict['keypoint'], (2, 1, 1, 1))\n",
    "    tmp_dict['keypoint_score'] = data[np.newaxis, :, :, 2] #because we do not have class 0\n",
    "    tmp_dict['keypoint_score'] = np.tile(tmp_dict['keypoint_score'], (2, 1, 1))\n",
    "    \n",
    "    data = transform(tmp_dict)\n",
    "    data = data['keypoint'][0]\n",
    "\n",
    "    data = data.numpy()\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we load the model\n",
    "\n",
    "model_weights_dir = './sampled_models_triplet/model_' + str(0.1) + '_' + str(0) + '.pth'\n",
    "\n",
    "# Load pre-trained weights to the backbone\n",
    "state_dict = os.path.join(script_dir, model_weights_dir)\n",
    "# load_checkpoint(model.backbone, backbone_state_dict)\n",
    "tmp = torch.load(state_dict)\n",
    "model.load_state_dict(tmp, strict=True)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all test data\n",
    "\n",
    "\n",
    "test_dataset_file = 'test_dataset14.npy'\n",
    "\n",
    "test_data = np.load(test_dataset_file)\n",
    "\n",
    "\n",
    "test_data_dict = {}\n",
    "for i in range(len(test_data)):\n",
    "    label, _, video, clip = test_data[i]\n",
    "    if video not in test_data_dict:\n",
    "        test_data_dict[video] = {}\n",
    "        test_data_dict[video]['label'] = label - 1\n",
    "        test_data_dict[video]['clip'] = []\n",
    "    test_data_dict[video]['clip'].append(clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings for training data\n",
    "training_dataset_file = 'sampled_trainingset/sampled_' + str(0.1) + '_' +str(0) + '.npy'\n",
    "training_data = np.load(training_dataset_file)\n",
    "embedding_dict = {}\n",
    "\n",
    "print('computing embeddings')\n",
    "for i in tqdm(range(len(training_data))):\n",
    "    label, _, video, clip = training_data[i]\n",
    "    label = label - 1\n",
    "\n",
    "    if label not in embedding_dict:\n",
    "        embedding_dict[label] = []\n",
    "    #load the sample \n",
    "    sample = load_sample(video, clip)\n",
    "    #get the embedding\n",
    "    sample = torch.from_numpy(sample).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        sample_result = model(sample.unsqueeze(0))\n",
    "    # normalize the embedding using F.normalize\n",
    "    sample_result = nn.functional.normalize(sample_result, p=2, dim=1)\n",
    "    sample_result = sample_result.cpu().detach().numpy().squeeze(0)\n",
    "    embedding_dict[label].append(sample_result)\n",
    "\n",
    "#convert each dict item to array\n",
    "for key in embedding_dict:\n",
    "    embedding_dict[key] = np.array(embedding_dict[key])\n",
    "\n",
    "# the shape of each array is (num_samples, 512)\n",
    "\n",
    "#build a dict of embeddings mapping to label\n",
    "lebel_dict = {}\n",
    "for key in embedding_dict:\n",
    "    for i in range(len(embedding_dict[key])):\n",
    "        lebel_dict[tuple(embedding_dict[key][i])] = key\n",
    "\n",
    "# get a 2d numpy array to put all embeddings together\n",
    "all_embeddings = np.zeros((0, 256))\n",
    "all_labels = []\n",
    "for key in embedding_dict:\n",
    "    all_embeddings = np.concatenate((all_embeddings, embedding_dict[key]), axis=0)\n",
    "    for i in range(len(embedding_dict[key])):\n",
    "        all_labels.append(key)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "#calculate the center of each class\n",
    "centers = np.zeros((4, 256))\n",
    "for key in embedding_dict:\n",
    "    centers[key] = np.mean(embedding_dict[key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the embeddings of testing data\n",
    "\n",
    "\n",
    "print('computing embeddings for testing data')\n",
    "test_embedding_dict = {}\n",
    "test_label_dict = {}\n",
    "for key in test_data_dict:\n",
    "    label = test_data_dict[key]['label']\n",
    "    clips = test_data_dict[key]['clip']\n",
    "    for c in clips:\n",
    "        # load the sample\n",
    "        sample = load_sample(key, c)\n",
    "        # get the embedding\n",
    "        sample = torch.from_numpy(sample).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            sample_result = model(sample.unsqueeze(0))\n",
    "        # normalize the embedding using F.normalize\n",
    "        sample_result = nn.functional.normalize(sample_result, p=2, dim=1)\n",
    "        sample_result = sample_result.cpu().detach().numpy().squeeze(0)\n",
    "        if key not in test_embedding_dict:\n",
    "            test_embedding_dict[key] = []\n",
    "            test_label_dict[key] = label\n",
    "        test_embedding_dict[key].append(sample_result)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training embeddings, labels, all embeddings, centers, \n",
    "# and testing embeddings, including the label\n",
    "# save everything in a single file\n",
    "\n",
    "\n",
    "save_dict = {}\n",
    "save_dict['training_embedding_dict'] = embedding_dict\n",
    "save_dict['training_label_dict'] = lebel_dict\n",
    "save_dict['all_embeddings'] = all_embeddings\n",
    "save_dict['all_labels'] = all_labels\n",
    "save_dict['centers'] = centers\n",
    "save_dict['test_embedding_dict'] = test_embedding_dict\n",
    "save_dict['test_label_dict'] = test_label_dict\n",
    "\n",
    "save_file = 'embeddings.npy'\n",
    "np.save(save_file, save_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9dde3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
